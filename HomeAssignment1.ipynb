{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1\n",
    "Due by 8th May, 2024 at 23:59 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "We want to create a 2 layer NN, which means we want to calculate  $y = W_2 * ReLU(W_1 * x + b_1) + b_2$\n",
    "\n",
    "Complete the TODOs below to create such a NN.\n",
    "\n",
    "Since you will be needing to compute the gradients w.r.t. all parameters, you may look into online resources for help. Please cite or link any online recources you do use.\n",
    "\n",
    "You are allowed to change any existing parts, however the code has to remain easy to understand and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "        \n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise ReLU application\n",
    "    \"\"\"\n",
    "    # return 1 / (1 + np.exp(-x)) # this is the sigmoid function, not ReLU\n",
    "    return np.maximum(0, x)  # ReLU function: max(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    # TODO: Write a function given a numpy array that calculates the gradient of the ReLU function w.r.t. `x`\n",
    "    # TODO: Also write the derivation of the gradient in the PDF file In the implementation you may simply use the final derivation.\n",
    "    # Hint: The function should return a numpy array of the same dimension that `x` has, but only containing 0 or 1\n",
    "    return np.where(x > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyNeuralNet:\n",
    "    \n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        # TODO: Randomly initialize the weight matrices W_1, W_2 and biases b_1, b_2\n",
    "        # Hint: use np.random.randn() and make sure to correctly set the dimensions \n",
    "        self.W_1 = np.random.randn(dim_hidden, dim_in) * np.sqrt(2.0 / dim_in)\n",
    "        self.W_2 = np.random.randn(dim_out, dim_hidden) * np.sqrt(2.0 / dim_hidden)\n",
    "        self.b_1 = np.random.randn(dim_hidden, 1) * np.sqrt(2.0 / dim_in)\n",
    "        self.b_2 = np.random.randn(dim_out, 1) * np.sqrt(2.0 / dim_hidden)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of the neural network for the given x.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): predicted output for `x`\n",
    "        \"\"\"\n",
    "        # TODO: Calculate output y\n",
    "        y = np.dot(self.W_2, relu(np.dot(self.W_1, x) + self.b_1)) + self.b_2\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the Mean-Squared Error and returns the gradients w.r.t. to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array with shape (self.dim_in, n)\n",
    "            y (np.ndarray): ground truth value numpy array with shape (self.dim_out, n)\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Mean-Squared-Error between predicted value on input points and ground truth value\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1   \n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2  \n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1   \n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2   \n",
    "        \"\"\"\n",
    "        y_pred = self.predict(x)\n",
    "        \n",
    "        # TODO: Calculate the loss (Mean-Squared-Error)\n",
    "        # Hint: use np.square() and np.mean()\n",
    "        loss = np.mean(np.square(y_pred - y))\n",
    "        \n",
    "        # TODO: Calculate all gradients w.r.t to the parameters\n",
    "        # Hint: You need to calculate the gradients for each of the parameters by hand\n",
    "        # Gradients of the loss w.r.t. y_pred\n",
    "        dL_dy_pred = 2 * (y_pred - y) / y.shape[1]\n",
    "        \n",
    "        # Gradients of the loss w.r.t. W_2 and b_2\n",
    "        dL_dW_2 = np.dot(dL_dy_pred, relu(np.dot(self.W_1, x) + self.b_1).T)\n",
    "        dL_db_2 = np.sum(dL_dy_pred, axis=1, keepdims=True)\n",
    "        \n",
    "        # Gradients of the loss w.r.t. the hidden layer\n",
    "        dL_dhidden = np.dot(self.W_2.T, dL_dy_pred) * relu_grad(np.dot(self.W_1, x) + self.b_1)\n",
    "        \n",
    "        # Gradients of the loss w.r.t. W_1 and b_1\n",
    "        dL_dW_1 = np.dot(dL_dhidden, x.T)\n",
    "        dL_db_1 = np.sum(dL_dhidden, axis=1, keepdims=True)\n",
    "        \n",
    "        return loss, dL_dW_1, dL_dW_2, dL_db_1, dL_db_2\n",
    "        \n",
    "        # TODO: Also write the derivation of the gradient in the PDF file. In the implementation you may simply use the final derivation.\n",
    "        \n",
    "         \n",
    "    def train(self, x, y, lr=0.001, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input values\n",
    "            y (np.ndarray): ground truth values\n",
    "            lr (float): learning rate, default: 0.001\n",
    "            epochs (int): number of epochs to train, default: 1000\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Return the loss achieved after all epochs\n",
    "        \"\"\"\n",
    "        # TODO: Keep track of the loss\n",
    "        loss_history = []\n",
    "        for i in range(epochs):\n",
    "            # TODO: Compute loss with x and update parameters of the model using SGD\n",
    "            loss, dL_dW_1, dL_dW_2, dL_db_1, dL_db_2 = self.loss(x, y)\n",
    "            self.W_1 -= lr * dL_dW_1\n",
    "            self.W_2 -= lr * dL_dW_2\n",
    "            self.b_1 -= lr * dL_db_1\n",
    "            self.b_2 -= lr * dL_db_2\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        # TODO: Plot the loss history and return the loss achieved after the final epoch\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(loss_history)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss History')\n",
    "        plt.show()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/27/qmbdy4fs531c8887h6nfgych0000gn/T/ipykernel_60517/1834695054.py:62: RuntimeWarning: invalid value encountered in multiply\n",
      "  dL_dhidden = np.dot(self.W_2.T, dL_dy_pred) * relu_grad(np.dot(self.W_1, x) + self.b_1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# We test the model created above on the simple function y = x^2\n",
    "\n",
    "model = NumPyNeuralNet(1, 20, 1)\n",
    "\n",
    "# Create a randomly distributed array of 1000 values\n",
    "x_train = 10 * np.random.randn(1, 1000)\n",
    "# Create ground truth by calculating x*x\n",
    "y_train = x_train * x_train\n",
    "\n",
    "loss = model.train(x_train, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### Intrinsic evaluation of embeddings\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes as input two words and computes the cosine similarity between them.\n",
    "You do not need to implement the cosine similarity calculation from scratch. Feel free to use any Python library.\n",
    "Remeber to ignore any pairs where at least one word is absent in the corpus. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    vec1 = wv[word1]\n",
    "    vec2 = wv[word2]\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Human (mean)\n",
       "0      love       sex          6.77\n",
       "1     tiger       cat          7.35\n",
       "2     tiger     tiger         10.00\n",
       "3      book     paper          7.46\n",
       "4  computer  keyboard          7.62"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming `word_pairs` is a list of tuples containing word pairs and their human similarity scores\n",
    "word_pairs_df = pd.read_csv('wordsim353_dataset.csv')\n",
    "\n",
    "word_pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word pairs: [('love', 'sex', 6.77), ('tiger', 'cat', 7.35), ('tiger', 'tiger', 10.0), ('book', 'paper', 7.46), ('computer', 'keyboard', 7.62)]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = list(zip(word_pairs_df['Word 1'], word_pairs_df['Word 2'], word_pairs_df['Human (mean)']))\n",
    "\n",
    "print(\"Word pairs:\", word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity for each word pair and store with their scores\n",
    "similarity_scores = []\n",
    "for word1, word2, human_score in word_pairs:\n",
    "    if word1 in wv and word2 in wv:\n",
    "        sim_score = similarity(word1, word2)\n",
    "        similarity_scores.append((word1, word2, sim_score, human_score))\n",
    "    else:\n",
    "        print(f\"Word not found in Word2Vec: {word1}, {word2}\")\n",
    "similarity_scores_df = pd.DataFrame(similarity_scores, columns=['Word 1', 'Word 2', 'Cosine Similarity', 'Human Score'])\n",
    "similarity_scores_df.to_csv('similarity_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Human Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>0.263938</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.517296</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>0.363463</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>0.396392</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Cosine Similarity  Human Score\n",
       "0      love       sex           0.263938         6.77\n",
       "1     tiger       cat           0.517296         7.35\n",
       "2     tiger     tiger           1.000000        10.00\n",
       "3      book     paper           0.363463         7.46\n",
       "4  computer  keyboard           0.396392         7.62"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word pairs in the list based on the human judgement scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tiger', 'tiger', 10.0), ('fuck', 'sex', 9.44), ('journey', 'voyage', 9.29), ('midday', 'noon', 9.29), ('dollar', 'buck', 9.22)]\n"
     ]
    }
   ],
   "source": [
    "sorted_word_pairs = sorted(word_pairs, key=lambda x: x[2], reverse=True)\n",
    "print(sorted_word_pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute spearman rank correlation between the two ranked lists obtained in the previous two steps.\n",
    "You do not need to implement the spearman rank correlation calculation from scratch. Feel free to use any Python library. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Rank Correlation: 0.7000166486272194\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute Spearman rank correlation\n",
    "human_scores = similarity_scores_df['Human Score']\n",
    "cosine_similarities = similarity_scores_df['Cosine Similarity']\n",
    "\n",
    "spearman_corr, _ = spearmanr(human_scores, cosine_similarities)\n",
    "print(f\"Spearman Rank Correlation: {spearman_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding based clasifier\n",
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n",
    "\n",
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fun',\n",
       " ',',\n",
       " 'Wasabi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'start',\n",
       " '.']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n",
    "    \n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    \"\"\"\n",
    "    Generate the embedding for a review\n",
    "    \"\"\"\n",
    "    embedding = np.zeros((300, 1))\n",
    "    for word in review.split():\n",
    "        if word in wv:\n",
    "            embedding += wv[word].reshape(-1, 1)\n",
    "    return embedding / len(review.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class for efficiently enumerating over the dataset. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "class sent_data(Dataset):\n",
    "    def __init__(self, X, y, max_len=100):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_len = max_len\n",
    "        self.word_to_id = defaultdict(lambda: len(self.word_to_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.X[idx]\n",
    "        token_ids = [self.word_to_id[word] for word in tokens]\n",
    "        padded = token_ids[:self.max_len] + [0] * (self.max_len - len(token_ids))\n",
    "        # **Return tensors** here\n",
    "        return torch.tensor(padded, dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                total_val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch}/{epochs} — Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set and report the test accuracy. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    # write your code snippet here\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    X_batch, y_batch = zip(*batch)\n",
    "    \n",
    "    # Stack X_batch into a single tensor\n",
    "    X_tensor = torch.stack(X_batch, dim=0)  # Stacks a list of tensors into a single tensor\n",
    "    y_tensor = torch.tensor(y_batch, dtype=torch.long)  # Convert y_batch into a tensor\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "\n",
    "model = Classifier(300, 100, 2)  # Adjust dimensions as needed\n",
    "train_dataset = sent_data(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataset = sent_data(val_X, val_y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_dataset = sent_data(test_X, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at embedding_lookup_idx.cc:214] 0 <= idx && idx < data_size. Index 0 is out of bounds: 9328, range 0 to 300",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[148], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# batch_X is a tensor [B, L]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     loss    \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[128], line 9\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textmining/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textmining/lib/python3.9/site-packages/torch/nn/modules/sparse.py:463\u001b[0m, in \u001b[0;36mEmbeddingBag.forward\u001b[0;34m(self, input, offsets, per_sample_weights)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    430\u001b[0m     offsets: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    431\u001b[0m     per_sample_weights: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of EmbeddingBag.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03m          returned vectors filled by zeros.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_bag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_last_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/textmining/lib/python3.9/site-packages/torch/nn/functional.py:2744\u001b[0m, in \u001b[0;36membedding_bag\u001b[0;34m(input, weight, offsets, max_norm, norm_type, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx)\u001b[0m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m per_sample_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   2739\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_bag: per_sample_weights was not None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2740\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_sample_weights is only supported for mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2741\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m). Please open a feature request on GitHub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2742\u001b[0m     )\n\u001b[0;32m-> 2744\u001b[0m ret, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_bag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode_enum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_last_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at embedding_lookup_idx.cc:214] 0 <= idx && idx < data_size. Index 0 is out of bounds: 9328, range 0 to 300"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, epochs=5, lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
