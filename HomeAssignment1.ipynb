{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1\n",
    "Due by 8th May, 2024 at 23:59 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "We want to create a 2 layer NN, which means we want to calculate  $y = W_2 * ReLU(W_1 * x + b_1) + b_2$\n",
    "\n",
    "Complete the TODOs below to create such a NN.\n",
    "\n",
    "Since you will be needing to compute the gradients w.r.t. all parameters, you may look into online resources for help. Please cite or link any online recources you do use.\n",
    "\n",
    "You are allowed to change any existing parts, however the code has to remain easy to understand and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "        \n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise ReLU application\n",
    "    \"\"\"\n",
    "    # return 1 / (1 + np.exp(-x)) # this is the sigmoid function, not ReLU\n",
    "    return np.maximum(0, x)  # ReLU function: max(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    # TODO: Write a function given a numpy array that calculates the gradient of the ReLU function w.r.t. `x`\n",
    "    # TODO: Also write the derivation of the gradient in the PDF file In the implementation you may simply use the final derivation.\n",
    "    # Hint: The function should return a numpy array of the same dimension that `x` has, but only containing 0 or 1\n",
    "    return np.where(x > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NumPyNeuralNet:\n",
    "    \n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        # TODO: Randomly initialize the weight matrices W_1, W_2 and biases b_1, b_2\n",
    "        # Hint: use np.random.randn() and make sure to correctly set the dimensions \n",
    "        self.W_1 = np.random.randn(dim_hidden, dim_in) * np.sqrt(2.0 / dim_in)\n",
    "        self.W_2 = np.random.randn(dim_out, dim_hidden) * np.sqrt(2.0 / dim_hidden)\n",
    "        self.b_1 = np.zeros((dim_hidden, 1))\n",
    "        self.b_2 = np.zeros((dim_out, 1))\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of the neural network for the given x.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): predicted output for `x`\n",
    "        \"\"\"\n",
    "        z1 = np.dot(self.W_1, x) + self.b_1\n",
    "        a1 = relu(z1)\n",
    "        y = np.dot(self.W_2, a1) + self.b_2\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the Mean-Squared Error and returns the gradients w.r.t. to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array with shape (self.dim_in, n)\n",
    "            y (np.ndarray): ground truth value numpy array with shape (self.dim_out, n)\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Mean-Squared-Error between predicted value on input points and ground truth value\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1   \n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2  \n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1   \n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2   \n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        z1 = np.dot(self.W_1, x) + self.b_1\n",
    "        a1 = relu(z1)\n",
    "        y_pred = np.dot(self.W_2, a1) + self.b_2\n",
    "        \n",
    "        # TODO: Calculate the loss (Mean-Squared-Error)\n",
    "        # Hint: use np.square() and np.mean()\n",
    "        loss = np.mean(np.square(y_pred - y))\n",
    "        \n",
    "        # backward pass\n",
    "        dL_dy_pred = 2 * (y_pred - y) / y.shape[1]\n",
    "        \n",
    "        # TODO: Calculate all gradients w.r.t to the parameters\n",
    "        # Hint: You need to calculate the gradients for each of the parameters by hand\n",
    "        # Gradients of the loss w.r.t. W_2 and b_2\n",
    "        dL_dW_2 = np.dot(dL_dy_pred, a1.T)\n",
    "        dL_db_2 = np.sum(dL_dy_pred, axis=1, keepdims=True)\n",
    "        \n",
    "        # Gradients of the loss w.r.t. the hidden layer\n",
    "        dL_dhidden = np.dot(self.W_2.T, dL_dy_pred) * relu_grad(z1)\n",
    "        \n",
    "        # Gradients of the loss w.r.t. W_1 and b_1\n",
    "        dL_dW_1 = np.dot(dL_dhidden, x.T)\n",
    "        dL_db_1 = np.sum(dL_dhidden, axis=1, keepdims=True)\n",
    "        \n",
    "        return loss, dL_dW_1, dL_dW_2, dL_db_1, dL_db_2\n",
    "        \n",
    "    def train(self, x, y, lr=0.0001, epochs=200):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input values\n",
    "            y (np.ndarray): ground truth values\n",
    "            lr (float): learning rate, default: 0.0001\n",
    "            epochs (int): number of epochs to train, default: 1000\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Return the loss achieved after all epochs\n",
    "        \"\"\"\n",
    "        # TODO: Keep track of the loss\n",
    "        loss_history = []\n",
    "        for i in range(epochs):\n",
    "            # TODO: Compute loss with x and update parameters of the model using SGD\n",
    "            loss, dL_dW_1, dL_dW_2, dL_db_1, dL_db_2 = self.loss(x, y)\n",
    "            self.W_1 -= lr * dL_dW_1\n",
    "            self.W_2 -= lr * dL_dW_2\n",
    "            self.b_1 -= lr * dL_db_1\n",
    "            self.b_2 -= lr * dL_db_2\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        # TODO: Plot the loss history and return the loss achieved after the final epoch\n",
    "        plt.plot(loss_history)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss History')\n",
    "        plt.show()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4432.300798164572\n"
     ]
    }
   ],
   "source": [
    "# We test the model created above on the simple function y = x^2\n",
    "\n",
    "model = NumPyNeuralNet(1, 20, 1)\n",
    "\n",
    "# Create a randomly distributed array of 1000 values\n",
    "x_train = 10 * np.random.randn(1, 1000)\n",
    "# Create ground truth by calculating x*x\n",
    "y_train = x_train * x_train\n",
    "\n",
    "loss = model.train(x_train, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### Intrinsic evaluation of embeddings\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes as input two words and computes the cosine similarity between them.\n",
    "You do not need to implement the cosine similarity calculation from scratch. Feel free to use any Python library.\n",
    "Remeber to ignore any pairs where at least one word is absent in the corpus. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    vec1 = wv[word1]\n",
    "    vec2 = wv[word2]\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Human (mean)\n",
       "0      love       sex          6.77\n",
       "1     tiger       cat          7.35\n",
       "2     tiger     tiger         10.00\n",
       "3      book     paper          7.46\n",
       "4  computer  keyboard          7.62"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming `word_pairs` is a list of tuples containing word pairs and their human similarity scores\n",
    "word_pairs_df = pd.read_csv('wordsim353_dataset.csv')\n",
    "\n",
    "word_pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word pairs: [('love', 'sex', 6.77), ('tiger', 'cat', 7.35), ('tiger', 'tiger', 10.0), ('book', 'paper', 7.46), ('computer', 'keyboard', 7.62)]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = list(zip(word_pairs_df['Word 1'], word_pairs_df['Word 2'], word_pairs_df['Human (mean)']))\n",
    "\n",
    "print(\"Word pairs:\", word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity for each word pair and store with their scores\n",
    "similarity_scores = []\n",
    "for word1, word2, human_score in word_pairs:\n",
    "    if word1 in wv and word2 in wv:\n",
    "        sim_score = similarity(word1, word2)\n",
    "        similarity_scores.append((word1, word2, sim_score, human_score))\n",
    "    else:\n",
    "        print(f\"Word not found in Word2Vec: {word1}, {word2}\")\n",
    "similarity_scores_df = pd.DataFrame(similarity_scores, columns=['Word 1', 'Word 2', 'Cosine Similarity', 'Human Score'])\n",
    "similarity_scores_df.to_csv('similarity_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Human Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>0.263938</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.517296</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>0.363463</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>0.396392</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Cosine Similarity  Human Score\n",
       "0      love       sex           0.263938         6.77\n",
       "1     tiger       cat           0.517296         7.35\n",
       "2     tiger     tiger           1.000000        10.00\n",
       "3      book     paper           0.363463         7.46\n",
       "4  computer  keyboard           0.396392         7.62"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word pairs in the list based on the human judgement scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tiger', 'tiger', 10.0), ('fuck', 'sex', 9.44), ('journey', 'voyage', 9.29), ('midday', 'noon', 9.29), ('dollar', 'buck', 9.22)]\n"
     ]
    }
   ],
   "source": [
    "sorted_word_pairs = sorted(word_pairs, key=lambda x: x[2], reverse=True)\n",
    "print(sorted_word_pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute spearman rank correlation between the two ranked lists obtained in the previous two steps.\n",
    "You do not need to implement the spearman rank correlation calculation from scratch. Feel free to use any Python library. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Rank Correlation: 0.7000166486272194\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute Spearman rank correlation\n",
    "human_scores = similarity_scores_df['Human Score']\n",
    "cosine_similarities = similarity_scores_df['Cosine Similarity']\n",
    "\n",
    "spearman_corr, _ = spearmanr(human_scores, cosine_similarities)\n",
    "print(f\"Spearman Rank Correlation: {spearman_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding based clasifier\n",
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n",
    "\n",
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fun',\n",
       " ',',\n",
       " 'Wasabi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'start',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n",
    "    \n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    vectors = [wv[word] for word in review if word in wv]\n",
    "    if not vectors:\n",
    "        print(f\"No vectors found for review: {review}\")\n",
    "        return np.zeros(300)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.Sequential = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class for efficiently enumerating over the dataset. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = [generate_embedding(review) for review in X]\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                total_val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch}/{epochs} — Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set and report the test accuracy. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    # write your code snippet here\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_data:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Calculate metrics for each batch\n",
    "            true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            false_negatives += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    return test_accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No vectors found for review: ['One-of-a-kind', 'near-masterpiece', '.']\n",
      "No vectors found for review: ['Execrable', '.']\n",
      "No vectors found for review: ['...', 'bibbidy-bobbidi-bland', '.']\n",
      "No vectors found for review: ['Wishy-washy', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "input: is the size of vector\n",
    "hidden: is the size of the hidden layer\n",
    "output: is the size of the output layer which is 2 for binary classification because we have two classes\n",
    "\"\"\"\n",
    "model = Classifier(300, 100, 2)\n",
    "train_dataset = SentimentDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = SentimentDataset(val_X, val_y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataset = SentimentDataset(test_X, test_y) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 — Validation Loss: 0.5008\n",
      "Epoch 2/200 — Validation Loss: 0.4843\n",
      "Epoch 3/200 — Validation Loss: 0.4405\n",
      "Epoch 4/200 — Validation Loss: 0.4399\n",
      "Epoch 5/200 — Validation Loss: 0.4686\n",
      "Epoch 6/200 — Validation Loss: 0.4371\n",
      "Epoch 7/200 — Validation Loss: 0.4378\n",
      "Epoch 8/200 — Validation Loss: 0.4351\n",
      "Epoch 9/200 — Validation Loss: 0.4354\n",
      "Epoch 10/200 — Validation Loss: 0.4369\n",
      "Epoch 11/200 — Validation Loss: 0.4481\n",
      "Epoch 12/200 — Validation Loss: 0.4345\n",
      "Epoch 13/200 — Validation Loss: 0.4369\n",
      "Epoch 14/200 — Validation Loss: 0.4549\n",
      "Epoch 15/200 — Validation Loss: 0.4487\n",
      "Epoch 16/200 — Validation Loss: 0.4448\n",
      "Epoch 17/200 — Validation Loss: 0.4438\n",
      "Epoch 18/200 — Validation Loss: 0.4404\n",
      "Epoch 19/200 — Validation Loss: 0.4453\n",
      "Epoch 20/200 — Validation Loss: 0.4440\n",
      "Epoch 21/200 — Validation Loss: 0.4520\n",
      "Epoch 22/200 — Validation Loss: 0.4543\n",
      "Epoch 23/200 — Validation Loss: 0.4647\n",
      "Epoch 24/200 — Validation Loss: 0.4507\n",
      "Epoch 25/200 — Validation Loss: 0.4529\n",
      "Epoch 26/200 — Validation Loss: 0.4717\n",
      "Epoch 27/200 — Validation Loss: 0.4547\n",
      "Epoch 28/200 — Validation Loss: 0.4643\n",
      "Epoch 29/200 — Validation Loss: 0.4670\n",
      "Epoch 30/200 — Validation Loss: 0.4761\n",
      "Epoch 31/200 — Validation Loss: 0.4797\n",
      "Epoch 32/200 — Validation Loss: 0.4831\n",
      "Epoch 33/200 — Validation Loss: 0.4769\n",
      "Epoch 34/200 — Validation Loss: 0.4705\n",
      "Epoch 35/200 — Validation Loss: 0.4753\n",
      "Epoch 36/200 — Validation Loss: 0.4929\n",
      "Epoch 37/200 — Validation Loss: 0.4909\n",
      "Epoch 38/200 — Validation Loss: 0.4922\n",
      "Epoch 39/200 — Validation Loss: 0.4930\n",
      "Epoch 40/200 — Validation Loss: 0.4852\n",
      "Epoch 41/200 — Validation Loss: 0.5028\n",
      "Epoch 42/200 — Validation Loss: 0.5068\n",
      "Epoch 43/200 — Validation Loss: 0.5192\n",
      "Epoch 44/200 — Validation Loss: 0.5045\n",
      "Epoch 45/200 — Validation Loss: 0.5024\n",
      "Epoch 46/200 — Validation Loss: 0.4997\n",
      "Epoch 47/200 — Validation Loss: 0.5087\n",
      "Epoch 48/200 — Validation Loss: 0.4960\n",
      "Epoch 49/200 — Validation Loss: 0.5106\n",
      "Epoch 50/200 — Validation Loss: 0.5159\n",
      "Epoch 51/200 — Validation Loss: 0.5164\n",
      "Epoch 52/200 — Validation Loss: 0.5288\n",
      "Epoch 53/200 — Validation Loss: 0.5283\n",
      "Epoch 54/200 — Validation Loss: 0.5194\n",
      "Epoch 55/200 — Validation Loss: 0.5287\n",
      "Epoch 56/200 — Validation Loss: 0.5229\n",
      "Epoch 57/200 — Validation Loss: 0.5217\n",
      "Epoch 58/200 — Validation Loss: 0.5344\n",
      "Epoch 59/200 — Validation Loss: 0.5484\n",
      "Epoch 60/200 — Validation Loss: 0.5434\n",
      "Epoch 61/200 — Validation Loss: 0.5474\n",
      "Epoch 62/200 — Validation Loss: 0.5418\n",
      "Epoch 63/200 — Validation Loss: 0.5488\n",
      "Epoch 64/200 — Validation Loss: 0.5705\n",
      "Epoch 65/200 — Validation Loss: 0.5654\n",
      "Epoch 66/200 — Validation Loss: 0.5686\n",
      "Epoch 67/200 — Validation Loss: 0.5749\n",
      "Epoch 68/200 — Validation Loss: 0.5682\n",
      "Epoch 69/200 — Validation Loss: 0.5870\n",
      "Epoch 70/200 — Validation Loss: 0.5839\n",
      "Epoch 71/200 — Validation Loss: 0.5706\n",
      "Epoch 72/200 — Validation Loss: 0.5803\n",
      "Epoch 73/200 — Validation Loss: 0.6007\n",
      "Epoch 74/200 — Validation Loss: 0.5988\n",
      "Epoch 75/200 — Validation Loss: 0.5961\n",
      "Epoch 76/200 — Validation Loss: 0.5967\n",
      "Epoch 77/200 — Validation Loss: 0.5943\n",
      "Epoch 78/200 — Validation Loss: 0.5967\n",
      "Epoch 79/200 — Validation Loss: 0.6168\n",
      "Epoch 80/200 — Validation Loss: 0.5983\n",
      "Epoch 81/200 — Validation Loss: 0.6262\n",
      "Epoch 82/200 — Validation Loss: 0.6026\n",
      "Epoch 83/200 — Validation Loss: 0.6099\n",
      "Epoch 84/200 — Validation Loss: 0.6135\n",
      "Epoch 85/200 — Validation Loss: 0.6216\n",
      "Epoch 86/200 — Validation Loss: 0.6461\n",
      "Epoch 87/200 — Validation Loss: 0.6195\n",
      "Epoch 88/200 — Validation Loss: 0.6256\n",
      "Epoch 89/200 — Validation Loss: 0.6282\n",
      "Epoch 90/200 — Validation Loss: 0.6412\n",
      "Epoch 91/200 — Validation Loss: 0.6313\n",
      "Epoch 92/200 — Validation Loss: 0.6543\n",
      "Epoch 93/200 — Validation Loss: 0.6321\n",
      "Epoch 94/200 — Validation Loss: 0.6465\n",
      "Epoch 95/200 — Validation Loss: 0.6502\n",
      "Epoch 96/200 — Validation Loss: 0.6532\n",
      "Epoch 97/200 — Validation Loss: 0.6605\n",
      "Epoch 98/200 — Validation Loss: 0.6677\n",
      "Epoch 99/200 — Validation Loss: 0.6769\n",
      "Epoch 100/200 — Validation Loss: 0.6819\n",
      "Epoch 101/200 — Validation Loss: 0.6642\n",
      "Epoch 102/200 — Validation Loss: 0.6562\n",
      "Epoch 103/200 — Validation Loss: 0.6889\n",
      "Epoch 104/200 — Validation Loss: 0.6651\n",
      "Epoch 105/200 — Validation Loss: 0.6781\n",
      "Epoch 106/200 — Validation Loss: 0.6948\n",
      "Epoch 107/200 — Validation Loss: 0.6823\n",
      "Epoch 108/200 — Validation Loss: 0.7023\n",
      "Epoch 109/200 — Validation Loss: 0.6930\n",
      "Epoch 110/200 — Validation Loss: 0.7062\n",
      "Epoch 111/200 — Validation Loss: 0.6855\n",
      "Epoch 112/200 — Validation Loss: 0.7042\n",
      "Epoch 113/200 — Validation Loss: 0.7259\n",
      "Epoch 114/200 — Validation Loss: 0.7247\n",
      "Epoch 115/200 — Validation Loss: 0.7355\n",
      "Epoch 116/200 — Validation Loss: 0.7099\n",
      "Epoch 117/200 — Validation Loss: 0.7413\n",
      "Epoch 118/200 — Validation Loss: 0.7242\n",
      "Epoch 119/200 — Validation Loss: 0.7368\n",
      "Epoch 120/200 — Validation Loss: 0.7381\n",
      "Epoch 121/200 — Validation Loss: 0.7423\n",
      "Epoch 122/200 — Validation Loss: 0.7635\n",
      "Epoch 123/200 — Validation Loss: 0.7492\n",
      "Epoch 124/200 — Validation Loss: 0.7745\n",
      "Epoch 125/200 — Validation Loss: 0.7714\n",
      "Epoch 126/200 — Validation Loss: 0.7644\n",
      "Epoch 127/200 — Validation Loss: 0.7630\n",
      "Epoch 128/200 — Validation Loss: 0.7514\n",
      "Epoch 129/200 — Validation Loss: 0.7594\n",
      "Epoch 130/200 — Validation Loss: 0.7642\n",
      "Epoch 131/200 — Validation Loss: 0.8001\n",
      "Epoch 132/200 — Validation Loss: 0.7794\n",
      "Epoch 133/200 — Validation Loss: 0.8038\n",
      "Epoch 134/200 — Validation Loss: 0.8020\n",
      "Epoch 135/200 — Validation Loss: 0.8036\n",
      "Epoch 136/200 — Validation Loss: 0.8018\n",
      "Epoch 137/200 — Validation Loss: 0.8042\n",
      "Epoch 138/200 — Validation Loss: 0.8231\n",
      "Epoch 139/200 — Validation Loss: 0.7942\n",
      "Epoch 140/200 — Validation Loss: 0.7958\n",
      "Epoch 141/200 — Validation Loss: 0.7938\n",
      "Epoch 142/200 — Validation Loss: 0.8300\n",
      "Epoch 143/200 — Validation Loss: 0.8149\n",
      "Epoch 144/200 — Validation Loss: 0.8417\n",
      "Epoch 145/200 — Validation Loss: 0.8264\n",
      "Epoch 146/200 — Validation Loss: 0.8291\n",
      "Epoch 147/200 — Validation Loss: 0.8397\n",
      "Epoch 148/200 — Validation Loss: 0.8232\n",
      "Epoch 149/200 — Validation Loss: 0.8333\n",
      "Epoch 150/200 — Validation Loss: 0.8405\n",
      "Epoch 151/200 — Validation Loss: 0.8480\n",
      "Epoch 152/200 — Validation Loss: 0.8702\n",
      "Epoch 153/200 — Validation Loss: 0.8752\n",
      "Epoch 154/200 — Validation Loss: 0.8579\n",
      "Epoch 155/200 — Validation Loss: 0.8695\n",
      "Epoch 156/200 — Validation Loss: 0.8960\n",
      "Epoch 157/200 — Validation Loss: 0.8944\n",
      "Epoch 158/200 — Validation Loss: 0.9016\n",
      "Epoch 159/200 — Validation Loss: 0.8866\n",
      "Epoch 160/200 — Validation Loss: 0.8960\n",
      "Epoch 161/200 — Validation Loss: 0.8927\n",
      "Epoch 162/200 — Validation Loss: 0.9134\n",
      "Epoch 163/200 — Validation Loss: 0.8873\n",
      "Epoch 164/200 — Validation Loss: 0.8853\n",
      "Epoch 165/200 — Validation Loss: 0.8930\n",
      "Epoch 166/200 — Validation Loss: 0.8926\n",
      "Epoch 167/200 — Validation Loss: 0.9135\n",
      "Epoch 168/200 — Validation Loss: 0.9452\n",
      "Epoch 169/200 — Validation Loss: 0.9179\n",
      "Epoch 170/200 — Validation Loss: 0.9203\n",
      "Epoch 171/200 — Validation Loss: 0.8937\n",
      "Epoch 172/200 — Validation Loss: 0.9136\n",
      "Epoch 173/200 — Validation Loss: 0.9503\n",
      "Epoch 174/200 — Validation Loss: 0.9243\n",
      "Epoch 175/200 — Validation Loss: 0.9156\n",
      "Epoch 176/200 — Validation Loss: 0.9448\n",
      "Epoch 177/200 — Validation Loss: 0.9444\n",
      "Epoch 178/200 — Validation Loss: 0.9620\n",
      "Epoch 179/200 — Validation Loss: 0.9517\n",
      "Epoch 180/200 — Validation Loss: 0.9491\n",
      "Epoch 181/200 — Validation Loss: 0.9579\n",
      "Epoch 182/200 — Validation Loss: 0.9655\n",
      "Epoch 183/200 — Validation Loss: 0.9576\n",
      "Epoch 184/200 — Validation Loss: 0.9797\n",
      "Epoch 185/200 — Validation Loss: 0.9817\n",
      "Epoch 186/200 — Validation Loss: 0.9433\n",
      "Epoch 187/200 — Validation Loss: 0.9843\n",
      "Epoch 188/200 — Validation Loss: 0.9629\n",
      "Epoch 189/200 — Validation Loss: 0.9732\n",
      "Epoch 190/200 — Validation Loss: 0.9797\n",
      "Epoch 191/200 — Validation Loss: 1.0153\n",
      "Epoch 192/200 — Validation Loss: 0.9899\n",
      "Epoch 193/200 — Validation Loss: 0.9940\n",
      "Epoch 194/200 — Validation Loss: 0.9843\n",
      "Epoch 195/200 — Validation Loss: 1.0190\n",
      "Epoch 196/200 — Validation Loss: 1.0199\n",
      "Epoch 197/200 — Validation Loss: 1.0066\n",
      "Epoch 198/200 — Validation Loss: 1.0083\n",
      "Epoch 199/200 — Validation Loss: 1.0148\n",
      "Epoch 200/200 — Validation Loss: 1.0218\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, epochs=200, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8088962108731467, 0.7961985216473073, 0.8294829482948295)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluations = evaluate(model, test_loader)\n",
    "print(evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
